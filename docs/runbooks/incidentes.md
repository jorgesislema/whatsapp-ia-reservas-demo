# Runbook: Respuesta a Incidentes

## Resumen
Procedimiento estandarizado para detectar, responder y resolver incidentes en producci√≥n.

## Audiencia
- On-call engineer
- Incident commander
- SRE team

## Severidad y Definiciones

| Severidad | Definici√≥n | Ejemplos | RTO |
|-----------|-----------|----------|-----|
| **SEV1** | Down total, sin workaround | Backend no responde, DB inaccesible | <15 min |
| **SEV2** | Degradation, usuarios afectados | Error rate 5-10%, latency spike | <30 min |
| **SEV3** | Anomal√≠a, funcionalidad limitada | M√©trica fuera de rango, error rate <1% | <2h |
| **SEV4** | Warning, sin impacto operacional | Disk usage high, test failure | <24h |

## Fase 1: Detecci√≥n (0-2 min)

### Alertas Autom√°ticas

Se disparan en 3 canales:

1. **PagerDuty** (cr√≠tico)
   - Webhook desde Cloud Monitoring
   - Llama on-call engineer
   
2. **Slack #incidents**
   - Mensaje autom√°tico con alerta + contexto
   - Enlace a dashboard
   
3. **Email** (backup)
   - ops@wa-team.com

### Se√±ales de Incidente

**Backend Metrics:**
```
- /healthz error rate > 5% ‚Üí SEV1
- Response latency P95 > 2s ‚Üí SEV2
- Error rate > 1% (pero < 5%) ‚Üí SEV3
- CPU > 90% sostenido ‚Üí SEV2
```

**Database Metrics:**
```
- Connections > 80% max ‚Üí SEV2
- Query latency spike > 3s ‚Üí SEV3
- Storage > 90% ‚Üí SEV2
```

**Manual Alerting:**
```
Usuarios reportan en Slack #support:
"No puedo hacer reservas desde hace 15 min"
‚Üí Verificar backend status
```

### Checklist de Detecci√≥n

- [ ] Alerta recibida (timestamp anotado)
- [ ] Severidad inicial estimada
- [ ] Dashboard abierto: https://console.cloud.google.com/monitoring
- [ ] Confirmaci√≥n: ¬ørealmente hay problema?

## Fase 2: Triage (2-5 min)

### Paso 1: Declarar Incidente

En Slack #incidents (autom√°tico o manual):
```
üö® INCIDENT DECLARED

Severidad: SEV2
Descripci√≥n: Error rate spike en backend
Iniciador: @carlos.lopez
Comenz√≥: 2024-01-15 14:30:00 UTC
Status: Investigating
```

### Paso 2: Incident Commander

- **SEV1:** Tech lead + SRE
- **SEV2:** SRE + backend engineer
- **SEV3:** Backend engineer solo
- **SEV4:** Documentar, no requiere coordinaci√≥n

Commander actualiza Slack cada 5 min.

### Paso 3: Diagnosis R√°pida (5 min m√°ximo)

```bash
# 1. Check backend status
curl https://wa-backend-xxxxx.a.run.app/healthz
# Expected: 200 OK + latency < 500ms

# 2. Check logs inmediatos (√∫ltimos 100 l√≠neas)
gcloud run services logs read wa-backend --limit=100

# 3. Check metrics en Cloud Console
# - Error rate (graph)
# - Response time (graph)
# - CPU / Memory usage

# 4. Check Cloud SQL
gcloud sql instances describe wa-db
# Status debe ser: RUNNABLE
# Connections: < 80% max

# 5. Check recientes deployments
gcloud run services list-revisions wa-backend --region=us-central1 | head -5
# ¬øHay revisi√≥n nueva hace < 30 min?
```

## Fase 3: Mitigaci√≥n Inmediata (5-15 min)

### Si Backend est√° Down (SEV1)

```bash
# Opci√≥n A: Rollback a revisi√≥n anterior
gcloud run services describe wa-backend --region=us-central1 \
  --format='table(status.traffic[].revisionName)'

PREV_REVISION="wa-backend-xxxxx"  # de salida anterior
gcloud run services update-traffic wa-backend \
  --to-revisions "$PREV_REVISION=100" \
  --region=us-central1

# Esperar 30 segundos
curl https://wa-backend-xxxxx.a.run.app/healthz

# Opci√≥n B: Restart service (si no hay rollback posible)
gcloud run services update wa-backend \
  --region=us-central1

# Opci√≥n C: Activar manual mode (si nada funciona)
gcloud run services update wa-backend \
  --set-env-vars="MANUAL_MODE=true" \
  --region=us-central1
# ‚Üí Bot no responde m√°s, usuarios reciben mensaje de fuera de servicio
```

### Si Database est√° Down (SEV1)

```bash
# Check status
gcloud sql instances describe wa-db

# Opci√≥n A: Restart instancia (detiene/inicia)
gcloud sql instances restart wa-db
# Esperar ~2-3 min

# Opci√≥n B: Failover a replica
# (Solo si ya tienes replica regional configurada en Paso 11)
gcloud sql instances failover wa-db --async

# Opci√≥n C: Restaurar desde backup (si corrupci√≥n)
# ‚Üí Ver 12.6 disaster_recovery.md
```

### Si Error Rate > 5% pero Services Respondiendo (SEV2)

```bash
# 1. Investigar qu√© endpoint falla
gcloud run services logs read wa-backend --limit=500 | \
  grep "ERROR\|Exception" | tail -20

# 2. Posibles causas + soluciones:
# - NLU service timeout ‚Üí Reintentar
# - DB timeout ‚Üí Check Cloud SQL
# - Memory leak ‚Üí Rollback
# - Rate limiting issue ‚Üí Check traffic

# 3. Si causado por reciente deployment ‚Üí Rollback
gcloud run services update-traffic wa-backend \
  --to-revisions "wa-backend-xxxxx=100" \
  --region=us-central1
```

## Fase 4: Monitoreo (durante mitigaci√≥n)

Abrir dashboard en paralelo:
```
https://console.cloud.google.com/monitoring/dashboards/custom/wa-dashboard
```

M√©tricas clave a monitorear:

```
Error rate: Objetivo < 1%
Response time P95: Objetivo < 800ms
CPU: Objetivo < 70%
Memory: Objetivo < 60%
```

Actualizar Slack cada 5 min:
```
üìä UPDATE (14:35)
- Rollback ejecutado
- Error rate: 4.2% ‚Üí 0.8% ‚úÖ
- Latency P95: 2500ms ‚Üí 650ms ‚úÖ
- Status: Monitoring
```

## Fase 5: Validaci√≥n (10-30 min post-incident)

### Smoke Tests

```bash
# 1. Verificar endpoints cr√≠ticos
curl https://wa-backend-xxxxx.a.run.app/healthz
curl https://wa-backend-xxxxx.a.run.app/metrics
curl https://wa-backend-xxxxx.a.run.app/config

# 2. Simular webhook t√≠pico
curl -X POST https://wa-backend-xxxxx.a.run.app/webhook \
  -H "X-Hub-Signature: sha256=XXXXX" \
  -H "Content-Type: application/json" \
  -d '{
    "object": "whatsapp_business_account",
    "entry": [{
      "changes": [{
        "value": {
          "messages": [{
            "from": "5939999999",
            "id": "test-msg-1",
            "timestamp": "1705349400",
            "text": {"body": "hola"}
          }]
        }
      }]
    }]
  }'
# Expected: 200 OK

# 3. Check DB queries
gcloud sql connect wa-db --user=root
> SELECT COUNT(*) FROM reservas WHERE created_at > NOW() - INTERVAL '1 hour';
# Debe haber movimiento reciente
```

### M√©tricas Post-Incident

- Error rate < 1% por 10 min consecutivos
- Latency P95 < 800ms por 10 min
- CPU < 70%, Memory < 60%
- No hay alertas activas

Si TODO ‚úÖ: Pasar a Fase 6 (postmortem)

## Fase 6: Notificaci√≥n & Cierre

### Declarar Resolved

```
Slack #incidents:
‚úÖ INCIDENT RESOLVED

Severidad: SEV2
Duraci√≥n: 8 minutos (14:30 - 14:38)
Causa Ra√≠z: Deployment 2024-01-15-1430 con bug en NLU slots
Acci√≥n: Rollback a revisi√≥n anterior
Impacto: ~45 mensajes sin procesar (recuperados despu√©s)
Pr√≥ximo Paso: Postmortem en 24h
```
```

### Escalaci√≥n a Gerencia

- **SEV1:** Notificaci√≥n inmediata a CTO
- **SEV2:** Notificaci√≥n despu√©s de resolve
- **SEV3+:** Log en spreadsheet, no notificaci√≥n

## Fase 7: Postmortem (24-48 h despu√©s)

### Template de Postmortem

```markdown
# Postmortem: [T√≠tulo del Incidente]

## Metadata
- Fecha: 2024-01-15
- Severidad: SEV2
- Duraci√≥n: 8 minutos
- Impacto: 45 mensajes

## Timeline
- **14:30:** Error rate spike detectada (alert disparada)
- **14:32:** On-call engineer notificado
- **14:33:** Investigaci√≥n ‚Üí encontrado deployment reciente
- **14:35:** Rollback ejecutado
- **14:36:** Error rate normalizada
- **14:38:** Incidente dado por resuelto

## Causa Ra√≠z (RCA)
Deployment 2024-01-15-1430 introdujo bug en NLU slot extraction.
El slot "party_size" no se extra√≠a correctamente, causando error en orchestrator.
Bug no fue detectado en testing porque test case no cubr√≠a variaci√≥n "para dos personas".

## Contributing Factors
1. PR review superficial (no not√≥ cambio en regex)
2. Testing no cubr√≠a todas las variaciones de language
3. Falta de E2E test en staging pre-deployment

## Qu√© Sali√≥ Bien
1. Alert disparada al toque (< 1 min)
2. On-call respondi√≥ r√°pido (< 2 min)
3. Rollback ejecutado exitosamente (< 5 min)
4. Comunicaci√≥n clara en Slack

## Qu√© Sali√≥ Mal
1. Test case insuficiente (no cubr√≠a "para dos personas")
2. PR review no fue exhaustivo
3. No se ejecut√≥ E2E en staging

## Acciones Correctivas

| Acci√≥n | Owner | Due Date | Status |
|--------|-------|----------|--------|
| Agregar test case para "para X personas" variaciones | @dev1 | 2024-01-20 | ‚òê |
| Mejorar PR review checklist (coverage requerido) | @lead | 2024-01-18 | ‚òê |
| Implementar E2E smoke test en staging (pre-deploy) | @qa | 2024-01-22 | ‚òê |

## Prevention (Future)
- Requerimiento: Coverage > 80% antes de PR merge
- Requerimiento: E2E test en staging antes de deploy
- Mejorar alerting: detectar cambios en PR que afecten NLU

## Lessons Learned
1. Edge cases en NLU son cr√≠ticos ‚Üí invertir en test coverage
2. Staging debe tener data realista + test automation
3. Rollback r√°pido es mejor que fix r√°pido (cuando hagas ambos: rollback primero)
```

### Revisar & Distribuir

1. Asignar due√±os de acciones correctivas
2. Compartir en Slack #postmortems
3. Mencionar en standup del equipo
4. Track completion de acciones (feedback loop)

## Escalaci√≥n & Contactos

| Escenario | Acci√≥n | Contacto |
|-----------|--------|----------|
| SEV1 > 5 min | Llamar CTO | +58-0412-XXXXX |
| SEV1 > 15 min | Llamar CEO | +58-0412-YYYYY |
| DB irrecuperable | Activar DR failover | SRE team |
| Ataque activo | Contactar security | security@wa-team.com |

## Recursos

- Cloud Run logs: `gcloud run services logs read wa-backend --limit=500`
- Cloud SQL console: https://console.cloud.google.com/sql
- Monitoring dashboard: https://console.cloud.google.com/monitoring
- Postmortem template: docs/runbooks/postmortem_template.md

**√öltima actualizaci√≥n:** 2024-01-15
**Pr√≥xima revisi√≥n:** 2024-02-15 (despu√©s de SGI training)
